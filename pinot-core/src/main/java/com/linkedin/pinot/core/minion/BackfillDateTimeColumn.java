/**
 * Copyright (C) 2014-2016 LinkedIn Corp. (pinot-core@linkedin.com)
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *         http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.linkedin.pinot.core.minion;

import com.google.common.base.Preconditions;
import com.google.common.collect.Sets;
import com.linkedin.pinot.common.data.DateTimeFieldSpec;
import com.linkedin.pinot.common.data.DateTimeFormatSpec;
import com.linkedin.pinot.common.data.DimensionFieldSpec;
import com.linkedin.pinot.common.data.MetricFieldSpec;
import com.linkedin.pinot.common.data.Schema;
import com.linkedin.pinot.common.data.StarTreeIndexSpec;
import com.linkedin.pinot.common.data.TimeFieldSpec;
import com.linkedin.pinot.common.data.TimeGranularitySpec;
import com.linkedin.pinot.common.segment.StarTreeMetadata;
import com.linkedin.pinot.core.data.GenericRow;
import com.linkedin.pinot.core.data.readers.BaseRecordReader;
import com.linkedin.pinot.core.data.readers.FileFormat;
import com.linkedin.pinot.core.data.readers.PinotSegmentRecordReader;
import com.linkedin.pinot.core.data.readers.RecordReader;
import com.linkedin.pinot.core.indexsegment.generator.SegmentGeneratorConfig;
import com.linkedin.pinot.core.segment.creator.RecordReaderSegmentCreationDataSource;
import com.linkedin.pinot.core.segment.creator.impl.SegmentIndexCreationDriverImpl;
import com.linkedin.pinot.core.segment.index.SegmentMetadataImpl;

import java.io.File;

import javax.annotation.Nonnull;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * The <code>BackfillDateTimeColumn</code> class takes a segment, a timeSpec from the segment, and a
 * dateTimeSpec.
 * It creates a new segment with a new column corresponding to the dateTimeSpec configs, using the values from the timeSpec
 * <ul>
 *  <li>If a column corresponding to the dateTimeSpec already exists, it is overwritten</li>
 *  <li>If not, a new date time column is created</li>
 *  <li>If the segment contains star tree, it is recreated, putting date time column at the end</li>
 * </ul>
 * <p>
 */
public class BackfillDateTimeColumn {
  private static final Logger LOGGER = LoggerFactory.getLogger(BackfillDateTimeColumn.class);

  private final File _originalIndexDir;
  private final File _backfilledIndexDir;
  private final TimeFieldSpec _srcTimeFieldSpec;
  private final DateTimeFieldSpec _destDateTimeFieldSpec;

  public BackfillDateTimeColumn(@Nonnull File originalIndexDir, @Nonnull File backfilledIndexDir,
      @Nonnull TimeFieldSpec srcTimeSpec, @Nonnull DateTimeFieldSpec destDateTimeSpec)
      throws Exception {
    _originalIndexDir = originalIndexDir;
    _backfilledIndexDir = backfilledIndexDir;
    Preconditions.checkArgument(!_originalIndexDir.getAbsolutePath().equals(_backfilledIndexDir.getAbsolutePath()),
        "Original index dir and backfill index dir should not be the same");
    _srcTimeFieldSpec = srcTimeSpec;
    _destDateTimeFieldSpec = destDateTimeSpec;
  }

  public boolean backfill() throws Exception {

    boolean success = true;
    SegmentMetadataImpl originalSegmentMetadata = new SegmentMetadataImpl(_originalIndexDir);
    String segmentName = originalSegmentMetadata.getName();
    String tableName = originalSegmentMetadata.getTableName();
    LOGGER.info("Start backfilling segment: {} in table: {}", segmentName, tableName);

    PinotSegmentRecordReader segmentRecordReader = new PinotSegmentRecordReader(_originalIndexDir);
    // read only rawdocs, everything after that, if present, is generated by star tree
    BackfillDateTimeRecordReader wrapperReader =
        new BackfillDateTimeRecordReader(segmentRecordReader, _srcTimeFieldSpec,
            _destDateTimeFieldSpec, originalSegmentMetadata.getTotalRawDocs());
    LOGGER.info("Segment dir: {} Output Dir: {}", _originalIndexDir.getAbsolutePath(),
        _backfilledIndexDir.getAbsolutePath());

    LOGGER.info("Creating segment generator config for {}", segmentName);
    SegmentGeneratorConfig config = new SegmentGeneratorConfig();
    config.setInputFilePath(_originalIndexDir.getAbsolutePath());
    config.setFormat(FileFormat.PINOT);
    config.setOutDir(_backfilledIndexDir.getAbsolutePath());
    config.setOverwrite(true);
    config.setTableName(tableName);
    config.setSegmentName(segmentName);
    config.setSchema(wrapperReader.getSchema());

    StarTreeMetadata starTreeMetadata = originalSegmentMetadata.getStarTreeMetadata();
    if (starTreeMetadata != null) {
      config.setEnableStarTreeIndex(true);
      StarTreeIndexSpec starTreeIndexSpec = new StarTreeIndexSpec();
      starTreeIndexSpec.setDimensionsSplitOrder(starTreeMetadata.getDimensionsSplitOrder());
      starTreeIndexSpec.setMaxLeafRecords((int) starTreeMetadata.getMaxLeafRecords());
      starTreeIndexSpec.setSkipMaterializationCardinalityThreshold((int) starTreeMetadata
          .getSkipMaterializationCardinality());
      starTreeIndexSpec.setSkipMaterializationForDimensions(Sets.newHashSet(starTreeMetadata
          .getSkipMaterializationForDimensions()));
      starTreeIndexSpec.setSkipStarNodeCreationForDimensions(Sets.newHashSet(starTreeMetadata
          .getSkipStarNodeCreationForDimensions()));
      config.setStarTreeIndexSpec(starTreeIndexSpec);
    }

    LOGGER.info("Creating segment for {} with config {}", segmentName, config.toString());
    SegmentIndexCreationDriverImpl driver = new SegmentIndexCreationDriverImpl();
    driver.init(config, new RecordReaderSegmentCreationDataSource(wrapperReader));
    driver.build();

    return success;
  }

  public BackfillDateTimeRecordReader getbackfillDateTimeRecordReader(RecordReader baseRecordReader) {
    return new BackfillDateTimeRecordReader(baseRecordReader, _srcTimeFieldSpec, _destDateTimeFieldSpec);
  }

  /**
   * This record reader is a wrapper over another record reader.
   * It simply reads the records from the base record reader, and adds a new field according to the
   * dateTimeFieldSpec
   */
  public class BackfillDateTimeRecordReader extends BaseRecordReader {

    private RecordReader _baseRecordReader;
    private TimeFieldSpec _timeFieldSpec;
    private DateTimeFieldSpec _dateTimeFieldSpec;
    private int _numRows = Integer.MAX_VALUE;
    private int _nextRow;

    /**
     * Constructor to read from the base record reader, but limit the number of rows read
     * This is useful when backfilling star tree segments, so that we read only total raw docs
     * @param baseRecordReader
     * @param timeFieldSpec
     * @param dateTimeFieldSpec
     * @param maxNumRows
     * @throws IOException
     * @throws ConfigurationException
     */
    public BackfillDateTimeRecordReader(RecordReader baseRecordReader, TimeFieldSpec timeFieldSpec,
        DateTimeFieldSpec dateTimeFieldSpec, int maxNumRows) {
      this(baseRecordReader, timeFieldSpec, dateTimeFieldSpec);
      _numRows = maxNumRows;
    }

    public BackfillDateTimeRecordReader(RecordReader baseRecordReader, TimeFieldSpec timeFieldSpec,
        DateTimeFieldSpec dateTimeFieldSpec) {
      _baseRecordReader = baseRecordReader;
      _timeFieldSpec = timeFieldSpec;
      _dateTimeFieldSpec = dateTimeFieldSpec;
      _nextRow = 0;
    }

    @Override
    public void init() throws Exception {
      _baseRecordReader.init();
      _nextRow = 0;
    }

    @Override
    public void rewind() throws Exception {
      _baseRecordReader.rewind();
      _nextRow = 0;
    }

    @Override
    public boolean hasNext() {
      return _nextRow < _numRows && _baseRecordReader.hasNext();
    }

    /**
     * Reads the schema from the baseRecordReader and adds/updates dateTimeFieldSpec to it, if not
     * already present {@inheritDoc}
     * @see com.linkedin.pinot.core.data.readers.RecordReader#getSchema()
     */
    @Override
    public Schema getSchema() {
      Schema schema = _baseRecordReader.getSchema();
      // base record reader doesn't have _dateTimeFieldSpec
      if (schema.getDateTimeNames() == null
          || !schema.getDateTimeNames().contains(_dateTimeFieldSpec.getName())) {
        schema.addField(_dateTimeFieldSpec);
      }
      // base record reader has dateTimeFieldSpec, but with different configs
      // create schema with updated dateTimeFieldSpec
      else if (!_dateTimeFieldSpec.equals(schema.getDateTimeSpec(_dateTimeFieldSpec.getName()))) {
        Schema newSchema = new Schema();
        for (DimensionFieldSpec dimensionFieldSpec : schema.getDimensionFieldSpecs()) {
          newSchema.addField(dimensionFieldSpec);
        }
        for (MetricFieldSpec metricFieldSpec : schema.getMetricFieldSpecs()) {
          newSchema.addField(metricFieldSpec);
        }
        newSchema.addField(schema.getTimeFieldSpec());
        for (DateTimeFieldSpec dateTimeFieldSpec : schema.getDateTimeFieldSpecs()) {
          if (!dateTimeFieldSpec.getName().equals(_dateTimeFieldSpec.getName())) {
            newSchema.addField(dateTimeFieldSpec);
          }
        }
        newSchema.addField(_dateTimeFieldSpec);
        newSchema.setSchemaName(schema.getSchemaName());
        return newSchema;
      }
      return schema;
    }

    @Override
    public GenericRow next() {
      return next(new GenericRow());
    }

    /**
     * Reads the next row from the baseRecordReader, and adds a dateTimeFieldSPec column to it
     * {@inheritDoc}
     * @see com.linkedin.pinot.core.data.readers.RecordReader#next(com.linkedin.pinot.core.data.GenericRow)
     */
    @Override
    public GenericRow next(GenericRow row) {
      _baseRecordReader.next(row);
      Long timeColumnValue = (Long) row.getValue(_timeFieldSpec.getIncomingTimeColumnName());
      Object dateTimeColumnValue = convertTimeFieldToDateTimeFieldSpec(timeColumnValue);
      row.putField(_dateTimeFieldSpec.getName(), dateTimeColumnValue);
      _nextRow++;
      return row;
    }

    /**
     * Converts the time column value from timeFieldSpec to dateTimeFieldSpec
     * @param timeColumnValue - time column value from timeFieldSpec
     * @return
     */
    private Object convertTimeFieldToDateTimeFieldSpec(Object timeColumnValue) {

      TimeGranularitySpec timeGranularitySpec = _timeFieldSpec.getOutgoingGranularitySpec();

      DateTimeFormatSpec formatFromTimeSpec =
          DateTimeFormatSpec.constructFormat(timeGranularitySpec.getTimeUnitSize(),
              timeGranularitySpec.getTimeType(), timeGranularitySpec.getTimeFormat());
      if (formatFromTimeSpec.getFormat().equals(_dateTimeFieldSpec.getFormat())) {
        return timeColumnValue;
      }

      long timeColumnValueMS = timeGranularitySpec.toMillis(timeColumnValue);
      DateTimeFormatSpec toFormat = new DateTimeFormatSpec(_dateTimeFieldSpec.getFormat());
      Object dateTimeColumnValue = toFormat.fromMillisToFormat(timeColumnValueMS, Object.class);
      return dateTimeColumnValue;
    }

    @Override
    public void close() throws Exception {
      _baseRecordReader.close();
    }
  }
}
